{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Open.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Herais/NLP_Learning_by_Selective_Data/blob/main/Open.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6SND0npASiN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b614727-b32d-4060-b0e4-8c2b908cfa6d"
      },
      "source": [
        "####[1]####\n",
        "# INTALL PROJECT FILES AND REQUIREMENTS\n",
        "# Cells to run before this one: None\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Navigate to project directory\n",
        "import os\n",
        "if not os.path.exists('/content/drive/MyDrive/Github'):\n",
        "  !mkdir Github\n",
        "path_github   = '/content/drive/MyDrive/Github'\n",
        "os.chdir(path_github)\n",
        "\n",
        "# git clone\n",
        "#!git clone https://github.com/Herais/NLP_Learning_by_Selective_Data.git\n",
        "path_wd = path_github + '/' + 'NLP_Learning_by_Selective_Data'\n",
        "os.chdir(path_wd)\n",
        "!pwd\n",
        "\n",
        "# INSTALL REQUIRED PACKAGES\n",
        "!pip install -r requirements.txt\n",
        "%tensorflow_version 1.x"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/Github/NLP_Learning_by_Selective_Data\n",
            "Collecting git+https://www.github.com/bojone/bert4keras.git@v0.8.3 (from -r requirements.txt (line 6))\n",
            "  Cloning https://www.github.com/bojone/bert4keras.git (to revision v0.8.3) to /tmp/pip-req-build-yzjamkmi\n",
            "  Running command git clone -q https://www.github.com/bojone/bert4keras.git /tmp/pip-req-build-yzjamkmi\n",
            "  Running command git checkout -q be69691e0ffdfd350d2d9fe501129a4b621d8a32\n",
            "Collecting rouge\n",
            "  Downloading https://files.pythonhosted.org/packages/43/cc/e18e33be20971ff73a056ebdb023476b5a545e744e3fc22acd8c758f1e0d/rouge-1.0.0-py3-none-any.whl\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (3.2.5)\n",
            "Collecting folium==0.2.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/72/dd/75ced7437bfa7cb9a88b96ee0177953062803c3b4cde411a97d98c35adaf/folium-0.2.1.tar.gz (69kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 10.9MB/s \n",
            "\u001b[?25hCollecting imgaug==0.2.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d2/60/a06a48d85a7e9062f5870347a3e3e953da30b37928d43b380c949bca458a/imgaug-0.2.5.tar.gz (562kB)\n",
            "\u001b[K     |████████████████████████████████| 563kB 35.5MB/s \n",
            "\u001b[?25hCollecting keras==2.3.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/fd/6bfe87920d7f4fd475acd28500a42482b6b84479832bdc0fe9e589a60ceb/Keras-2.3.1-py2.py3-none-any.whl (377kB)\n",
            "\u001b[K     |████████████████████████████████| 378kB 42.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from rouge->-r requirements.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: Jinja2 in /usr/local/lib/python3.7/dist-packages (from folium==0.2.1->-r requirements.txt (line 3)) (2.11.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from imgaug==0.2.5->-r requirements.txt (line 4)) (1.4.1)\n",
            "Requirement already satisfied: scikit-image>=0.11.0 in /usr/local/lib/python3.7/dist-packages (from imgaug==0.2.5->-r requirements.txt (line 4)) (0.16.2)\n",
            "Requirement already satisfied: numpy>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from imgaug==0.2.5->-r requirements.txt (line 4)) (1.19.5)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1->-r requirements.txt (line 5)) (2.10.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1->-r requirements.txt (line 5)) (3.13)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1->-r requirements.txt (line 5)) (1.1.2)\n",
            "Collecting keras-applications>=1.0.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 7.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2->folium==0.2.1->-r requirements.txt (line 3)) (2.0.0)\n",
            "Requirement already satisfied: pillow>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.11.0->imgaug==0.2.5->-r requirements.txt (line 4)) (7.1.2)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.11.0->imgaug==0.2.5->-r requirements.txt (line 4)) (2.4.1)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.11.0->imgaug==0.2.5->-r requirements.txt (line 4)) (2.5.1)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.11.0->imgaug==0.2.5->-r requirements.txt (line 4)) (1.1.1)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.11.0->imgaug==0.2.5->-r requirements.txt (line 4)) (3.2.2)\n",
            "Requirement already satisfied: decorator<5,>=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx>=2.0->scikit-image>=0.11.0->imgaug==0.2.5->-r requirements.txt (line 4)) (4.4.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug==0.2.5->-r requirements.txt (line 4)) (2.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug==0.2.5->-r requirements.txt (line 4)) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug==0.2.5->-r requirements.txt (line 4)) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug==0.2.5->-r requirements.txt (line 4)) (0.10.0)\n",
            "Building wheels for collected packages: folium, imgaug, bert4keras\n",
            "  Building wheel for folium (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for folium: filename=folium-0.2.1-cp37-none-any.whl size=79979 sha256=b01b05ebd9a66494fd485ee01558b1823fdeb6d64b4a24bd5436afe24f9e151a\n",
            "  Stored in directory: /root/.cache/pip/wheels/b8/09/f0/52d2ef419c2aaf4fb149f92a33e0008bdce7ae816f0dd8f0c5\n",
            "  Building wheel for imgaug (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for imgaug: filename=imgaug-0.2.5-cp37-none-any.whl size=561439 sha256=ff0cbf15ab0a343d1bcef90bdcee9306f51edcfc5f7e23daab55137cbee42839\n",
            "  Stored in directory: /root/.cache/pip/wheels/31/48/c8/ca3345e8582a078de94243996e148377ef66fdb845557bae0b\n",
            "  Building wheel for bert4keras (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bert4keras: filename=bert4keras-0.8.3-cp37-none-any.whl size=41796 sha256=af167b071ee82f64eea7d90f5b1690234e37d1198f9074f62babe419d4ba1304\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-iy2vu0xy/wheels/ba/89/2b/e84eb8164743a8f97dc5518184797b30dbfaa0a01b1e3edb6b\n",
            "Successfully built folium imgaug bert4keras\n",
            "Installing collected packages: rouge, folium, imgaug, keras-applications, keras, bert4keras\n",
            "  Found existing installation: folium 0.8.3\n",
            "    Uninstalling folium-0.8.3:\n",
            "      Successfully uninstalled folium-0.8.3\n",
            "  Found existing installation: imgaug 0.2.9\n",
            "    Uninstalling imgaug-0.2.9:\n",
            "      Successfully uninstalled imgaug-0.2.9\n",
            "  Found existing installation: Keras 2.4.3\n",
            "    Uninstalling Keras-2.4.3:\n",
            "      Successfully uninstalled Keras-2.4.3\n",
            "Successfully installed bert4keras-0.8.3 folium-0.2.1 imgaug-0.2.5 keras-2.3.1 keras-applications-1.0.8 rouge-1.0.0\n",
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xpzb_n4g__-W"
      },
      "source": [
        "####[2]####\n",
        "# LOAD AND EXPLORE DATASETS\n",
        "# Cells to run before this one: [1]\n",
        "# Note: does not need GPU/TPU\n",
        "\n",
        "# Import Libaries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Path\n",
        "path_datasets ='/datasets'\n",
        "\n",
        "# Load Dataset: Movie Title and Summary\n",
        "path_dataset_movie = '/Movie_Title_and_Summary'\n",
        "df_movie = pd.read_json(path_wd + path_datasets + path_dataset_movie + '/train.json', orient='records', lines=True)\n",
        "df_movie.head(10)\n",
        "\n",
        "# Load Dataset: Computer Research Literature (CSL) Title and Abstract\n",
        "path_dataset_csl = '/CSL'\n",
        "df_csl = pd.read_json(path_wd + path_datasets + path_dataset_csl + '/train_new_2500.json', orient='records', lines=True)\n",
        "df_csl.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHCi6ImzT9Zn"
      },
      "source": [
        "####[3]####\n",
        "# LOAD TRAINED MODELS\n",
        "# Cells to run before this one: [1]\n",
        "# [1] Mount Drive, Install Requirements\n",
        "\n",
        "# Notes: Used load_weights rather than load_model here for simplicity to avoid defining customlayer,\n",
        "# First reconstruct the model structure, then load trained weights.\n",
        "\n",
        "# Load Libraries\n",
        "from tensorflow import keras\n",
        "from bert4keras.models import build_transformer_model\n",
        "\n",
        "# Set Path\n",
        "#path_models = '/models'\n",
        "path_weights = '/weights'\n",
        "path_pretrained_model = '/pretrained_model'\n",
        "\n",
        "# Reconstruct Model Parameter to Load Weights\n",
        "########################################\n",
        "# Set Model Parameters\n",
        "epochs = 10 # number of epochs to train\n",
        "batch_size = 16 # \n",
        "maxlen = 256\n",
        "topk = 1\n",
        "\n",
        "# Set Pretrained Model Parameters (BERT)\n",
        "config_path = path_wd + path_pretrained_model + '/chinese_L-12_H-768_A-12/bert_config.json'\n",
        "checkpoint_path = path_wd + path_pretrained_model + '/chinese_L-12_H-768_A-12/bert_model.ckpt'\n",
        "dict_path = path_wd + path_pretrained_model + '/chinese_L-12_H-768_A-12/vocab.txt'\n",
        "\n",
        "# Load Vocab\n",
        "from bert4keras.tokenizers import Tokenizer, load_vocab\n",
        "\n",
        "token_dict, keep_tokens = load_vocab(\n",
        "    dict_path=dict_path,\n",
        "    simplified=True,\n",
        "    startswith=['[PAD]', '[UNK]', '[CLS]', '[SEP]'],\n",
        ")\n",
        "tokenizer = Tokenizer(token_dict, do_lower_case=True)\n",
        "########################################\n",
        "\n",
        "# the Star Chaser Model trained with movie dataset only\n",
        "model_StarChaser = build_transformer_model(\n",
        "    config_path,\n",
        "    checkpoint_path,\n",
        "    application='unilm',\n",
        "    keep_tokens=keep_tokens,  # include only tokens in keep tokens\n",
        ")\n",
        "path_model_weights = path_wd + path_weights + '/StarChaser_best_model.weights'\n",
        "model_StarChaser.load_weights(path_model_weights)\n",
        "\n",
        "# the Scholar Model trained with Computer Science Literature only\n",
        "model_Scholar = build_transformer_model(\n",
        "    config_path,\n",
        "    checkpoint_path,\n",
        "    application='unilm',\n",
        "    keep_tokens=keep_tokens,   # include only tokens in keep tokens\n",
        ")\n",
        "path_model_weights = path_wd + path_weights + '/Scholar2_best_model.weights'\n",
        "model_StarChaser.load_weights(path_model_weights)\n",
        "\n",
        "# the Know-it-all Model Trained with both movie and cscl dataset\n",
        "model_Knowitall = build_transformer_model(\n",
        "    config_path,\n",
        "    checkpoint_path,\n",
        "    application='unilm',\n",
        "    keep_tokens=keep_tokens,   # include only tokens in keep tokens\n",
        ") \n",
        "path_model_weights = path_wd + path_weights + '/Knowitall2_best_model.weights'\n",
        "model_StarChaser.load_weights(path_model_weights)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zf8FM0X_UqAW"
      },
      "source": [
        "####[4]####\n",
        "# MAKE PREDICTIONS\n",
        "# Cells to run before this one:  [1] and [3], or [1]+[5]+[6]\n",
        "# [1] Mount Drive, Install Requirements\n",
        "# [3] Load Models\n",
        "# [5] Train Model - initialize model for trainning\n",
        "# [6] Train Model - Training and save weights\n",
        "\n",
        "# Load Library\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "from bert4keras.snippets import DataGenerator, AutoRegressiveDecoder\n",
        "\n",
        "# Set Path\n",
        "path_datasets ='/datasets'\n",
        "path_predictions = '/predictions'\n",
        "path_dataset_movie = '/Movie_Title_and_Summary'\n",
        "path_dataset_csl = '/CSL'\n",
        "path_test = path_wd + path_datasets + path_dataset_csl + '/test.json'\n",
        "path_prediction = path_wd + path_predictions + '/model_Knowitall3_predicts_CSLTitles.json'\n",
        "\n",
        "# Load Test Data\n",
        "test_data = []\n",
        "with open(path_test, encoding='utf-8') as f:\n",
        "    for l in f:\n",
        "        test_data.append(json.loads(l.strip()))\n",
        "\n",
        "# Decoder: seq2seq\n",
        "class AutoTitle(AutoRegressiveDecoder):\n",
        "    #seq2seq decoder\n",
        "    @AutoRegressiveDecoder.wraps(default_rtype='probas')\n",
        "    def predict(self, inputs, output_ids, states):\n",
        "        token_ids, segment_ids = inputs\n",
        "        token_ids = np.concatenate([token_ids, output_ids], 1)\n",
        "        segment_ids = np.concatenate([segment_ids, np.ones_like(output_ids)], 1)\n",
        "        # CHANGE MODEL NAME HERE TO SWITCH PREDICTING MODELS:\n",
        "        #####################\n",
        "        return model.predict([token_ids, segment_ids])[:, -1]\n",
        "        #####################\n",
        "\n",
        "    def generate(self, text, topk=1):\n",
        "        max_c_len = maxlen - self.maxlen\n",
        "        token_ids, segment_ids = tokenizer.encode(text, maxlen=max_c_len)\n",
        "        output_ids = self.beam_search([token_ids, segment_ids],\n",
        "                                      topk=topk)  # beam search\n",
        "        return tokenizer.decode(output_ids)\n",
        "\n",
        "autotitle = AutoTitle(start_id=None, end_id=tokenizer._token_end_id, maxlen=32)\n",
        "\n",
        "# Write to Prediction Path\n",
        "with open(path_prediction ,'w') as f:\n",
        "    for data in test_data:\n",
        "        pred_title = ''.join(autotitle.generate(data[\"abst\"], topk)).lower()\n",
        "        output = {}\n",
        "        output[\"id\"] = data[\"id\"]\n",
        "        output[\"title\"] = pred_title\n",
        "        f.write(json.dumps(output, ensure_ascii = False))\n",
        "        f.write('\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j43nGvQiVULh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40c53f67-53a1-4c6e-f830-ec08d64eeb4e"
      },
      "source": [
        "####[5]####\n",
        "# TRAIN MORE MODELS - CONFIGURE\n",
        "# Cells to run before this one: [1]\n",
        "# [1] Mount Drive, Install Requirements\n",
        "\n",
        "# Load Libraries\n",
        "from __future__ import print_function\n",
        "import numpy as np\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "from bert4keras.backend import keras, K\n",
        "from bert4keras.layers import Loss\n",
        "from bert4keras.models import build_transformer_model\n",
        "from bert4keras.tokenizers import Tokenizer, load_vocab\n",
        "from bert4keras.optimizers import Adam\n",
        "from bert4keras.snippets import sequence_padding, open\n",
        "from bert4keras.snippets import DataGenerator, AutoRegressiveDecoder\n",
        "from keras.models import Model\n",
        "from rouge import Rouge\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "import utils  # LocalLibraries\n",
        "\n",
        "# Set Path\n",
        "path_models = '/models'\n",
        "path_datasets ='/datasets'\n",
        "path_weights = '/weights'\n",
        "\n",
        "# Set Name of this model\n",
        "########################################\n",
        "model_name = 'Knowitall3'\n",
        "########################################\n",
        "\n",
        "# Load Train and Dev Data\n",
        "path_train = path_wd + path_datasets + '/CSL_Movie_Title_and_Summary/train.json'\n",
        "path_dev = path_wd + path_datasets + '/CSL_Movie_Title_and_Summary/val.json'\n",
        "train_data = utils.load_data(path_train)\n",
        "valid_data = utils.load_data(path_dev)\n",
        "# Could use pandas, but pandas has limitation on cell string length\n",
        "\n",
        "# Set Model Parameter\n",
        "epochs = 10\n",
        "batch_size = 16\n",
        "maxlen = 256\n",
        "topk = 1\n",
        "\n",
        "# Set BERT Path\n",
        "# Download pretrained BERT model if necesary\n",
        "path_pretrained_model = '/pretrained_model'\n",
        "path_bert_chinese = path_wd + path_pretrained_model + '/chinese_L-12_H-768_A-12'\n",
        "if not os.path.exists(path_bert_chinese):\n",
        " # Download Model\n",
        "  !wget https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip -P './pretrained_model/'\n",
        "  %cd pretrained_model\n",
        "  !unzip chinese_L-12_H-768_A-12.zip\n",
        "  !rm -f chinese_L-12_H-768_A-12.zip\n",
        "  os.chdir(path_wd)\n",
        "  !pwd\n",
        "config_path = path_wd + path_pretrained_model + '/chinese_L-12_H-768_A-12/bert_config.json'\n",
        "checkpoint_path = path_wd + path_pretrained_model + '/chinese_L-12_H-768_A-12/bert_model.ckpt'\n",
        "dict_path = path_wd + path_pretrained_model + '/chinese_L-12_H-768_A-12/vocab.txt'\n",
        "\n",
        "# Load Vocab and Tokenize\n",
        "token_dict, keep_tokens = load_vocab(\n",
        "    dict_path=dict_path,\n",
        "    simplified=True,\n",
        "    startswith=['[PAD]', '[UNK]', '[CLS]', '[SEP]'],\n",
        ")\n",
        "tokenizer = Tokenizer(token_dict, do_lower_case=True)\n",
        " \n",
        "# Data Generator\n",
        "class data_generator(DataGenerator):\n",
        "    \"\"\"数据生成器\n",
        "    \"\"\"\n",
        "    def __iter__(self, random=False):\n",
        "        batch_token_ids, batch_segment_ids = [], []\n",
        "        for is_end, (title, content) in self.sample(random):\n",
        "            token_ids, segment_ids = tokenizer.encode(\n",
        "                content, title, maxlen=maxlen\n",
        "            )\n",
        "            batch_token_ids.append(token_ids)\n",
        "            batch_segment_ids.append(segment_ids)\n",
        "            if len(batch_token_ids) == self.batch_size or is_end:\n",
        "                batch_token_ids = sequence_padding(batch_token_ids)\n",
        "                batch_segment_ids = sequence_padding(batch_segment_ids)\n",
        "                yield [batch_token_ids, batch_segment_ids], None\n",
        "                batch_token_ids, batch_segment_ids = [], []\n",
        "train_generator = data_generator(train_data, batch_size)\n",
        "\n",
        "# Loss Computation\n",
        "class CrossEntropy(Loss):\n",
        "    \"\"\"交叉熵作为loss，并mask掉输入部分\n",
        "    \"\"\"\n",
        "    def compute_loss(self, inputs, mask=None):\n",
        "        y_true, y_mask, y_pred = inputs\n",
        "        y_true = y_true[:, 1:]  # 目标token_ids\n",
        "        y_mask = y_mask[:, 1:]  # segment_ids，刚好指示了要预测的部分\n",
        "        y_pred = y_pred[:, :-1]  # 预测序列，错开一位\n",
        "        loss = K.sparse_categorical_crossentropy(y_true, y_pred)\n",
        "        loss = K.sum(loss * y_mask) / K.sum(y_mask)\n",
        "        return loss\n",
        "\n",
        "# Initialize Model with BERT\n",
        "model = build_transformer_model(\n",
        "    config_path,\n",
        "    checkpoint_path,\n",
        "    application='unilm',\n",
        "    keep_tokens=keep_tokens,  # reduce vocab size\n",
        ")\n",
        "output = CrossEntropy(2)(model.inputs + model.outputs)\n",
        "\n",
        "# Load Previous Weights for Continued Training\n",
        "#path_model_weights = path_wd + path_weights + '/ScholarNew_best_model.weights'\n",
        "#model.load_weights(path_model_weights)\n",
        "######\n",
        "model = Model(model.inputs, output)\n",
        "model.compile(optimizer=Adam(1e-5))\n",
        "model.summary()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "Input-Token (InputLayer)        (None, None)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Input-Segment (InputLayer)      (None, None)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Token (Embedding)     multiple             10432512    Input-Token[0][0]                \n",
            "                                                                 MLM-Norm[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Segment (Embedding)   (None, None, 768)    1536        Input-Segment[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Token-Segment (Add)   (None, None, 768)    0           Embedding-Token[0][0]            \n",
            "                                                                 Embedding-Segment[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Position (PositionEmb (None, None, 768)    393216      Embedding-Token-Segment[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Norm (LayerNormalizat (None, None, 768)    1536        Embedding-Position[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Dropout (Dropout)     (None, None, 768)    0           Embedding-Norm[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "Attention-UniLM-Mask (Lambda)   (None, 1, None, None 0           Input-Segment[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-0-MultiHeadSelfAtte (None, None, 768)    2362368     Embedding-Dropout[0][0]          \n",
            "                                                                 Embedding-Dropout[0][0]          \n",
            "                                                                 Embedding-Dropout[0][0]          \n",
            "                                                                 Attention-UniLM-Mask[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-0-MultiHeadSelfAtte (None, None, 768)    0           Transformer-0-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-0-MultiHeadSelfAtte (None, None, 768)    0           Embedding-Dropout[0][0]          \n",
            "                                                                 Transformer-0-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-0-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-0-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-0-FeedForward (Feed (None, None, 768)    4722432     Transformer-0-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-0-FeedForward-Dropo (None, None, 768)    0           Transformer-0-FeedForward[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-0-FeedForward-Add ( (None, None, 768)    0           Transformer-0-MultiHeadSelfAttent\n",
            "                                                                 Transformer-0-FeedForward-Dropout\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-0-FeedForward-Norm  (None, None, 768)    1536        Transformer-0-FeedForward-Add[0][\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-1-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-0-FeedForward-Norm[0]\n",
            "                                                                 Transformer-0-FeedForward-Norm[0]\n",
            "                                                                 Transformer-0-FeedForward-Norm[0]\n",
            "                                                                 Attention-UniLM-Mask[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-1-MultiHeadSelfAtte (None, None, 768)    0           Transformer-1-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-1-MultiHeadSelfAtte (None, None, 768)    0           Transformer-0-FeedForward-Norm[0]\n",
            "                                                                 Transformer-1-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-1-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-1-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-1-FeedForward (Feed (None, None, 768)    4722432     Transformer-1-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-1-FeedForward-Dropo (None, None, 768)    0           Transformer-1-FeedForward[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-1-FeedForward-Add ( (None, None, 768)    0           Transformer-1-MultiHeadSelfAttent\n",
            "                                                                 Transformer-1-FeedForward-Dropout\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-1-FeedForward-Norm  (None, None, 768)    1536        Transformer-1-FeedForward-Add[0][\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-2-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-1-FeedForward-Norm[0]\n",
            "                                                                 Transformer-1-FeedForward-Norm[0]\n",
            "                                                                 Transformer-1-FeedForward-Norm[0]\n",
            "                                                                 Attention-UniLM-Mask[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-2-MultiHeadSelfAtte (None, None, 768)    0           Transformer-2-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-2-MultiHeadSelfAtte (None, None, 768)    0           Transformer-1-FeedForward-Norm[0]\n",
            "                                                                 Transformer-2-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-2-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-2-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-2-FeedForward (Feed (None, None, 768)    4722432     Transformer-2-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-2-FeedForward-Dropo (None, None, 768)    0           Transformer-2-FeedForward[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-2-FeedForward-Add ( (None, None, 768)    0           Transformer-2-MultiHeadSelfAttent\n",
            "                                                                 Transformer-2-FeedForward-Dropout\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-2-FeedForward-Norm  (None, None, 768)    1536        Transformer-2-FeedForward-Add[0][\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-3-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-2-FeedForward-Norm[0]\n",
            "                                                                 Transformer-2-FeedForward-Norm[0]\n",
            "                                                                 Transformer-2-FeedForward-Norm[0]\n",
            "                                                                 Attention-UniLM-Mask[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-3-MultiHeadSelfAtte (None, None, 768)    0           Transformer-3-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-3-MultiHeadSelfAtte (None, None, 768)    0           Transformer-2-FeedForward-Norm[0]\n",
            "                                                                 Transformer-3-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-3-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-3-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-3-FeedForward (Feed (None, None, 768)    4722432     Transformer-3-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-3-FeedForward-Dropo (None, None, 768)    0           Transformer-3-FeedForward[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-3-FeedForward-Add ( (None, None, 768)    0           Transformer-3-MultiHeadSelfAttent\n",
            "                                                                 Transformer-3-FeedForward-Dropout\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-3-FeedForward-Norm  (None, None, 768)    1536        Transformer-3-FeedForward-Add[0][\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-4-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-3-FeedForward-Norm[0]\n",
            "                                                                 Transformer-3-FeedForward-Norm[0]\n",
            "                                                                 Transformer-3-FeedForward-Norm[0]\n",
            "                                                                 Attention-UniLM-Mask[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-4-MultiHeadSelfAtte (None, None, 768)    0           Transformer-4-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-4-MultiHeadSelfAtte (None, None, 768)    0           Transformer-3-FeedForward-Norm[0]\n",
            "                                                                 Transformer-4-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-4-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-4-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-4-FeedForward (Feed (None, None, 768)    4722432     Transformer-4-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-4-FeedForward-Dropo (None, None, 768)    0           Transformer-4-FeedForward[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-4-FeedForward-Add ( (None, None, 768)    0           Transformer-4-MultiHeadSelfAttent\n",
            "                                                                 Transformer-4-FeedForward-Dropout\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-4-FeedForward-Norm  (None, None, 768)    1536        Transformer-4-FeedForward-Add[0][\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-5-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-4-FeedForward-Norm[0]\n",
            "                                                                 Transformer-4-FeedForward-Norm[0]\n",
            "                                                                 Transformer-4-FeedForward-Norm[0]\n",
            "                                                                 Attention-UniLM-Mask[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-5-MultiHeadSelfAtte (None, None, 768)    0           Transformer-5-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-5-MultiHeadSelfAtte (None, None, 768)    0           Transformer-4-FeedForward-Norm[0]\n",
            "                                                                 Transformer-5-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-5-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-5-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-5-FeedForward (Feed (None, None, 768)    4722432     Transformer-5-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-5-FeedForward-Dropo (None, None, 768)    0           Transformer-5-FeedForward[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-5-FeedForward-Add ( (None, None, 768)    0           Transformer-5-MultiHeadSelfAttent\n",
            "                                                                 Transformer-5-FeedForward-Dropout\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-5-FeedForward-Norm  (None, None, 768)    1536        Transformer-5-FeedForward-Add[0][\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-6-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-5-FeedForward-Norm[0]\n",
            "                                                                 Transformer-5-FeedForward-Norm[0]\n",
            "                                                                 Transformer-5-FeedForward-Norm[0]\n",
            "                                                                 Attention-UniLM-Mask[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-6-MultiHeadSelfAtte (None, None, 768)    0           Transformer-6-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-6-MultiHeadSelfAtte (None, None, 768)    0           Transformer-5-FeedForward-Norm[0]\n",
            "                                                                 Transformer-6-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-6-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-6-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-6-FeedForward (Feed (None, None, 768)    4722432     Transformer-6-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-6-FeedForward-Dropo (None, None, 768)    0           Transformer-6-FeedForward[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-6-FeedForward-Add ( (None, None, 768)    0           Transformer-6-MultiHeadSelfAttent\n",
            "                                                                 Transformer-6-FeedForward-Dropout\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-6-FeedForward-Norm  (None, None, 768)    1536        Transformer-6-FeedForward-Add[0][\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-7-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-6-FeedForward-Norm[0]\n",
            "                                                                 Transformer-6-FeedForward-Norm[0]\n",
            "                                                                 Transformer-6-FeedForward-Norm[0]\n",
            "                                                                 Attention-UniLM-Mask[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-7-MultiHeadSelfAtte (None, None, 768)    0           Transformer-7-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-7-MultiHeadSelfAtte (None, None, 768)    0           Transformer-6-FeedForward-Norm[0]\n",
            "                                                                 Transformer-7-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-7-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-7-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-7-FeedForward (Feed (None, None, 768)    4722432     Transformer-7-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-7-FeedForward-Dropo (None, None, 768)    0           Transformer-7-FeedForward[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-7-FeedForward-Add ( (None, None, 768)    0           Transformer-7-MultiHeadSelfAttent\n",
            "                                                                 Transformer-7-FeedForward-Dropout\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-7-FeedForward-Norm  (None, None, 768)    1536        Transformer-7-FeedForward-Add[0][\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-8-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-7-FeedForward-Norm[0]\n",
            "                                                                 Transformer-7-FeedForward-Norm[0]\n",
            "                                                                 Transformer-7-FeedForward-Norm[0]\n",
            "                                                                 Attention-UniLM-Mask[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-8-MultiHeadSelfAtte (None, None, 768)    0           Transformer-8-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-8-MultiHeadSelfAtte (None, None, 768)    0           Transformer-7-FeedForward-Norm[0]\n",
            "                                                                 Transformer-8-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-8-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-8-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-8-FeedForward (Feed (None, None, 768)    4722432     Transformer-8-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-8-FeedForward-Dropo (None, None, 768)    0           Transformer-8-FeedForward[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-8-FeedForward-Add ( (None, None, 768)    0           Transformer-8-MultiHeadSelfAttent\n",
            "                                                                 Transformer-8-FeedForward-Dropout\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-8-FeedForward-Norm  (None, None, 768)    1536        Transformer-8-FeedForward-Add[0][\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-9-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-8-FeedForward-Norm[0]\n",
            "                                                                 Transformer-8-FeedForward-Norm[0]\n",
            "                                                                 Transformer-8-FeedForward-Norm[0]\n",
            "                                                                 Attention-UniLM-Mask[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-9-MultiHeadSelfAtte (None, None, 768)    0           Transformer-9-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-9-MultiHeadSelfAtte (None, None, 768)    0           Transformer-8-FeedForward-Norm[0]\n",
            "                                                                 Transformer-9-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-9-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-9-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-9-FeedForward (Feed (None, None, 768)    4722432     Transformer-9-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-9-FeedForward-Dropo (None, None, 768)    0           Transformer-9-FeedForward[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-9-FeedForward-Add ( (None, None, 768)    0           Transformer-9-MultiHeadSelfAttent\n",
            "                                                                 Transformer-9-FeedForward-Dropout\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-9-FeedForward-Norm  (None, None, 768)    1536        Transformer-9-FeedForward-Add[0][\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-10-MultiHeadSelfAtt (None, None, 768)    2362368     Transformer-9-FeedForward-Norm[0]\n",
            "                                                                 Transformer-9-FeedForward-Norm[0]\n",
            "                                                                 Transformer-9-FeedForward-Norm[0]\n",
            "                                                                 Attention-UniLM-Mask[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-10-MultiHeadSelfAtt (None, None, 768)    0           Transformer-10-MultiHeadSelfAtten\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-10-MultiHeadSelfAtt (None, None, 768)    0           Transformer-9-FeedForward-Norm[0]\n",
            "                                                                 Transformer-10-MultiHeadSelfAtten\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-10-MultiHeadSelfAtt (None, None, 768)    1536        Transformer-10-MultiHeadSelfAtten\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-10-FeedForward (Fee (None, None, 768)    4722432     Transformer-10-MultiHeadSelfAtten\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-10-FeedForward-Drop (None, None, 768)    0           Transformer-10-FeedForward[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-10-FeedForward-Add  (None, None, 768)    0           Transformer-10-MultiHeadSelfAtten\n",
            "                                                                 Transformer-10-FeedForward-Dropou\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-10-FeedForward-Norm (None, None, 768)    1536        Transformer-10-FeedForward-Add[0]\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-11-MultiHeadSelfAtt (None, None, 768)    2362368     Transformer-10-FeedForward-Norm[0\n",
            "                                                                 Transformer-10-FeedForward-Norm[0\n",
            "                                                                 Transformer-10-FeedForward-Norm[0\n",
            "                                                                 Attention-UniLM-Mask[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-11-MultiHeadSelfAtt (None, None, 768)    0           Transformer-11-MultiHeadSelfAtten\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-11-MultiHeadSelfAtt (None, None, 768)    0           Transformer-10-FeedForward-Norm[0\n",
            "                                                                 Transformer-11-MultiHeadSelfAtten\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-11-MultiHeadSelfAtt (None, None, 768)    1536        Transformer-11-MultiHeadSelfAtten\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-11-FeedForward (Fee (None, None, 768)    4722432     Transformer-11-MultiHeadSelfAtten\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-11-FeedForward-Drop (None, None, 768)    0           Transformer-11-FeedForward[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-11-FeedForward-Add  (None, None, 768)    0           Transformer-11-MultiHeadSelfAtten\n",
            "                                                                 Transformer-11-FeedForward-Dropou\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-11-FeedForward-Norm (None, None, 768)    1536        Transformer-11-FeedForward-Add[0]\n",
            "__________________________________________________________________________________________________\n",
            "MLM-Dense (Dense)               (None, None, 768)    590592      Transformer-11-FeedForward-Norm[0\n",
            "__________________________________________________________________________________________________\n",
            "MLM-Norm (LayerNormalization)   (None, None, 768)    1536        MLM-Dense[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "MLM-Bias (BiasAdd)              (None, None, 13584)  13584       Embedding-Token[1][0]            \n",
            "__________________________________________________________________________________________________\n",
            "MLM-Activation (Activation)     (None, None, 13584)  0           MLM-Bias[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "cross_entropy_1 (CrossEntropy)  (None, None, 13584)  0           Input-Token[0][0]                \n",
            "                                                                 Input-Segment[0][0]              \n",
            "                                                                 MLM-Activation[0][0]             \n",
            "==================================================================================================\n",
            "Total params: 96,488,976\n",
            "Trainable params: 96,488,976\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/tensorflow-1.15.2/python3.7/keras/engine/training_utils.py:819: UserWarning: Output cross_entropy_1 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to cross_entropy_1.\n",
            "  'be expecting any data to be passed to {0}.'.format(name))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5eMkEEfBf-0"
      },
      "source": [
        "####[6]####\n",
        "# TRAIN MORE MODELS - TRAINING\n",
        "# Cells to run before this one: [1],[4]\n",
        "# [1] Mount Drive, Install Requirements\n",
        "# [4] Train more models - config\n",
        "\n",
        "# Decoder: seq2seq\n",
        "class AutoTitle(AutoRegressiveDecoder):\n",
        "    #seq2seq decoder\n",
        "    @AutoRegressiveDecoder.wraps(default_rtype='probas')\n",
        "    def predict(self, inputs, output_ids, states):\n",
        "        token_ids, segment_ids = inputs\n",
        "        token_ids = np.concatenate([token_ids, output_ids], 1)\n",
        "        segment_ids = np.concatenate([segment_ids, np.ones_like(output_ids)], 1)\n",
        "        # CHANGE MODEL NAME HERE TO SWITCH PREDICTING MODELS:\n",
        "        return model.predict([token_ids, segment_ids])[:, -1]\n",
        "\n",
        "    def generate(self, text, topk=1):\n",
        "        max_c_len = maxlen - self.maxlen\n",
        "        token_ids, segment_ids = tokenizer.encode(text, maxlen=max_c_len)\n",
        "        output_ids = self.beam_search([token_ids, segment_ids],\n",
        "                                      topk=topk)  # beam search\n",
        "        return tokenizer.decode(output_ids)\n",
        "autotitle = AutoTitle(start_id=None, end_id=tokenizer._token_end_id, maxlen=32)\n",
        "\n",
        "# Callback\n",
        "class Evaluator(keras.callbacks.Callback):\n",
        "    def __init__(self, model_name=''):\n",
        "        self.rouge = Rouge()\n",
        "        self.smooth = SmoothingFunction().method1\n",
        "        self.best_bleu = 0.\n",
        "        self.valid_data = valid_data\n",
        "        self.model_name = model_name # save weights given model name\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        metrics = self.evaluate(valid_data)  # 评测模型\n",
        "        if metrics['bleu'] > self.best_bleu:\n",
        "            self.best_bleu = metrics['bleu']\n",
        "            model.save_weights('./weights/' + self.model_name + '_best_model.weights')  # 保存模型\n",
        "        metrics['best_bleu'] = self.best_bleu\n",
        "        print('valid_data:', metrics)\n",
        "\n",
        "    def evaluate(self, data, topk=1):\n",
        "        total = 0\n",
        "        rouge_1, rouge_2, rouge_l, bleu = 0, 0, 0, 0\n",
        "        for title, content in tqdm(data):\n",
        "            total += 1\n",
        "            title = ' '.join(title).lower()\n",
        "            pred_title = ' '.join(autotitle.generate(content, topk)).lower()\n",
        "            if pred_title.strip():\n",
        "                scores = self.rouge.get_scores(hyps=pred_title, refs=title)\n",
        "                rouge_1 += scores[0]['rouge-1']['f']\n",
        "                rouge_2 += scores[0]['rouge-2']['f']\n",
        "                rouge_l += scores[0]['rouge-l']['f']\n",
        "                bleu += sentence_bleu(\n",
        "                    references=[title.split(' ')],\n",
        "                    hypothesis=pred_title.split(' '),\n",
        "                    smoothing_function=self.smooth\n",
        "                )\n",
        "        rouge_1 /= total\n",
        "        rouge_2 /= total\n",
        "        rouge_l /= total\n",
        "        bleu /= total\n",
        "        return {\n",
        "            'rouge-1': rouge_1,\n",
        "            'rouge-2': rouge_2,\n",
        "            'rouge-l': rouge_l,\n",
        "            'bleu': bleu,\n",
        "        }\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUruqO_Xkkx_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4cbe6c2-1008-46b0-d125-1966965e363e"
      },
      "source": [
        "####[6]####\n",
        "# TRAIN MORE MODELS - TRAINING - Includes resetting path for continuous training\n",
        "# Cells to run before this one: [1],[4]，[6]\n",
        "# [1] Mount Drive, Install Requirements\n",
        "# [4] Train more models - config\n",
        "# [6] Run only once, then use this for continued training\n",
        "\n",
        "# Load Train and Dev Data Here for Continued Training\n",
        "########################################\n",
        "#model_name = 'Knowitall'\n",
        "########################################\n",
        "#path_train = path_wd + path_datasets + '/Movie_Title_and_Summary/train.json'\n",
        "#path_dev = path_wd + path_datasets + '/Movie_Title_and_Summary/val.json'\n",
        "#train_data = utils.load_data(path_train)\n",
        "#valid_data = utils.load_data(path_dev)\n",
        "\n",
        "evaluator = Evaluator(model_name)\n",
        "train_generator = data_generator(train_data, batch_size)\n",
        "\n",
        "# Train Model\n",
        "model.fit(\n",
        "    train_generator.forfit(),\n",
        "    steps_per_epoch=len(train_generator),\n",
        "    epochs=epochs,\n",
        "    callbacks=[evaluator]\n",
        ")\n",
        "\n",
        "# Save Trained Model\n",
        "#utils.save_model_with_dt(path_wd + path_models, model, model_name)\n",
        "# Alternatively best weights are saved under ./weights"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "Epoch 1/10\n",
            "1402/1402 [==============================] - 1149s 820ms/step - loss: 3.3779\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5483/5483 [08:26<00:00, 10.82it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "valid_data: {'rouge-1': 0.24753894474090923, 'rouge-2': 0.17177220265852694, 'rouge-l': 0.24749489972439445, 'bleu': 0.1162587092141086, 'best_bleu': 0.1162587092141086}\n",
            "Epoch 2/10\n",
            "1180/1402 [========================>.....] - ETA: 3:02 - loss: 2.6018"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTk-bQxYUxn5"
      },
      "source": [
        "####[6]####\n",
        "# LOAD PREDICTIONS\n",
        "# Cells to run before this one: [1]\n",
        "# [1] Mount Drive, Install Requirements\n",
        "# Note: does not need GPU/TPU, for more analysis, ue Result.ipynb\n",
        "\n",
        "# Load Results\n",
        "path_predictions = '/predictions'\n",
        "path_model_StarChaser_predicts_MovieTitles = '/model_StarChaser_predicts_MovieTitles.json'\n",
        "path_model_StarChaser_predicts_CSLTitles = '/model_StarChaser_predicts_CSLTitles.json'\n",
        "path_model_Scholar_predicts_CSLTitles = '/model_Scholar_predicts_CSLTitles.json'\n",
        "path_model_Scholar_predicts_MovieTitles = '/model_Scholar_predicts_MovieTitles.json'\n",
        "path_model_Knowitall_predicts_CSLTitles = '/model_Knowitall_predicts_CSLTitles.json'\n",
        "path_model_Knowitall_predicts_MovieTitles = '/model_Knowitall_predicts_MovieTitles.json'"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}