{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Open.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Herais/NLP_Learning_by_Selective_Data/blob/main/Open.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6SND0npASiN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f82fdccd-6f9a-4941-c7c1-f3652b2f95b3"
      },
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Navigate to project directory\n",
        "import os\n",
        "if not os.path.exists('/content/drive/MyDrive/Github'):\n",
        "  !mkdir Github\n",
        "path_github   = '/content/drive/MyDrive/Github'\n",
        "os.chdir(path_github)\n",
        "\n",
        "# git clone\n",
        "#!git clone https://github.com/Herais/NLP_Learning_by_Selective_Data.git\n",
        "path_wd = path_github + '/' + 'NLP_Learning_by_Selective_Data'\n",
        "os.chdir(path_wd)\n",
        "!pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive/Github/NLP_Learning_by_Selective_Data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-lUgX-tsT9Q"
      },
      "source": [
        "# INSTALL REQUIRED PACKAGES\n",
        "!pip install -r requirements.txt\n",
        "%tensorflow_version 1.x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xpzb_n4g__-W",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 529
        },
        "outputId": "5f52664a-c6e8-4724-e7a5-24499cc75759"
      },
      "source": [
        "# LOAD AND EXPLORE DATASETS\n",
        "\n",
        "# Import Libaries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Path\n",
        "path_datasets ='/datasets'\n",
        "\n",
        "# Load Dataset: Movie Title and Summary\n",
        "path_dataset_movie = '/Movie_Title_and_Summary'\n",
        "df_movie = pd.read_json(path_wd + path_datasets + path_dataset_movie + '/train.json', orient='records', lines=True)\n",
        "df_movie.head(10)\n",
        "\n",
        "# Load Dataset: Computer Research Literature (CSL) Title and Abstract\n",
        "path_dataset_csl = '/CSL'\n",
        "df_csl = pd.read_json(path_wd + path_datasets + path_dataset_csl + '/train.json', orient='records', lines=True)\n",
        "df_csl.head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>title</th>\n",
              "      <th>abst</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>保细节的网格刚性变形算法</td>\n",
              "      <td>提出了一种新的保细节的变形算法,可以使网格模型进行尽量刚性的变形,以减少变形中几何细节的扭曲...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>一种基于混合模型的实时虚拟人服装动画方法</td>\n",
              "      <td>实时服装动画生成技术能够为三维虚拟角色实时地生成逼真的服装动态效果,在游戏娱乐、虚拟服装设计...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>人脸遮挡区域检测与重建</td>\n",
              "      <td>提出一种基于模糊主分量分析技术(FPCA)的人脸遮挡检测与去除方法.首先,有遮挡人脸被投影到...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>一种基于奇异值分解的图像匹配算法</td>\n",
              "      <td>图像匹配技术在计算机视觉、遥感和医学图像分析等领域有着广泛的应用背景.针对传统的相关匹配算法...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>片相似性各项异性扩散图像去噪</td>\n",
              "      <td>提出了一种基于片相似性的各项异性扩散图像去噪方法.传统的各项异性图像去噪方法都是基于单个像素...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>6</td>\n",
              "      <td>基于软硬数据的多点地质统计法在图像统计信息重构中的应用研究</td>\n",
              "      <td>仅使用硬数据或无条件数据时,图像统计信息的重构会比较困难而且精度不高.如果在重构过程中加入软...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>7</td>\n",
              "      <td>一种基于自适应区域分割的地形模型简化方法</td>\n",
              "      <td>鉴于统一误差计算模型简化方法自适应性差的问题,提出了一种根据地势特征自适应分割地形区域从而有...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>8</td>\n",
              "      <td>基于样例的交互式三维动画的生成</td>\n",
              "      <td>在基于草图的三维动画复制的基础上,提出了基于样例的交互式三维动画生成方法.在保留源动画基本风...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>9</td>\n",
              "      <td>一种挖掘压缩序列模式的有效算法</td>\n",
              "      <td>从序列数据库中挖掘频繁序列模式是数据挖掘领域的一个中心研究主题,而且该领域已经提出和研究了各...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>10</td>\n",
              "      <td>基于KL距离的非平衡数据半监督学习算法</td>\n",
              "      <td>在实际应用中,由于各种原因时常无法直接获得已标识反例,导致传统分类方法暂时失灵,因此,基于正...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id  ...                                               abst\n",
              "0   1  ...  提出了一种新的保细节的变形算法,可以使网格模型进行尽量刚性的变形,以减少变形中几何细节的扭曲...\n",
              "1   2  ...  实时服装动画生成技术能够为三维虚拟角色实时地生成逼真的服装动态效果,在游戏娱乐、虚拟服装设计...\n",
              "2   3  ...  提出一种基于模糊主分量分析技术(FPCA)的人脸遮挡检测与去除方法.首先,有遮挡人脸被投影到...\n",
              "3   4  ...  图像匹配技术在计算机视觉、遥感和医学图像分析等领域有着广泛的应用背景.针对传统的相关匹配算法...\n",
              "4   5  ...  提出了一种基于片相似性的各项异性扩散图像去噪方法.传统的各项异性图像去噪方法都是基于单个像素...\n",
              "5   6  ...  仅使用硬数据或无条件数据时,图像统计信息的重构会比较困难而且精度不高.如果在重构过程中加入软...\n",
              "6   7  ...  鉴于统一误差计算模型简化方法自适应性差的问题,提出了一种根据地势特征自适应分割地形区域从而有...\n",
              "7   8  ...  在基于草图的三维动画复制的基础上,提出了基于样例的交互式三维动画生成方法.在保留源动画基本风...\n",
              "8   9  ...  从序列数据库中挖掘频繁序列模式是数据挖掘领域的一个中心研究主题,而且该领域已经提出和研究了各...\n",
              "9  10  ...  在实际应用中,由于各种原因时常无法直接获得已标识反例,导致传统分类方法暂时失灵,因此,基于正...\n",
              "\n",
              "[10 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRJEApk43tm-"
      },
      "source": [
        "# SET MODEL PARAMETERS\n",
        "\n",
        "# Set Model Parameter\n",
        "epochs = 10\n",
        "batch_size = 8\n",
        "maxlen = 256\n",
        "topk = 1\n",
        "\n",
        "# Download pretrained BERT model if necesary\n",
        "path_pretrained_model = '/pretrained_model'\n",
        "path_bert_chinese = path_wd + path_pretrained_model + '/chinese_L-12_H-768_A-12'\n",
        "\n",
        "if not os.path.exists(path_bert_chinese):\n",
        " # Download Model\n",
        "  !wget https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip -P './pretrained_model/'\n",
        "  %cd pretrained_model\n",
        "  !unzip chinese_L-12_H-768_A-12.zip\n",
        "  !rm -f chinese_L-12_H-768_A-12.zip\n",
        "  os.chdir(path_wd)\n",
        "  !pwd\n",
        "\n",
        "# Set Pretrained Model Parameters (BERT)\n",
        "config_path = path_wd + path_pretrained_model + '/chinese_L-12_H-768_A-12/bert_config.json'\n",
        "checkpoint_path = path_wd + path_pretrained_model + '/chinese_L-12_H-768_A-12/bert_model.ckpt'\n",
        "dict_path = path_wd + path_pretrained_model + '/chinese_L-12_H-768_A-12/vocab.txt'\n",
        "\n",
        "\n",
        "# Load Vocab\n",
        "from bert4keras.tokenizers import Tokenizer, load_vocab\n",
        "\n",
        "token_dict, keep_tokens = load_vocab(\n",
        "    dict_path=dict_path,\n",
        "    simplified=True,\n",
        "    startswith=['[PAD]', '[UNK]', '[CLS]', '[SEP]'],\n",
        ")\n",
        "tokenizer = Tokenizer(token_dict, do_lower_case=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j43nGvQiVULh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "d6ff6b3c-4e3c-43c5-cebb-0837c2a56d83"
      },
      "source": [
        "\n",
        "# TRAIN MODELS\n",
        "# Skip to LOAD TRAINED MODELS to use existing models for prediction\n",
        "\n",
        "# Load Libraries\n",
        "from __future__ import print_function\n",
        "import numpy as np\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "from bert4keras.backend import keras, K\n",
        "from bert4keras.layers import Loss\n",
        "from bert4keras.models import build_transformer_model\n",
        "from bert4keras.tokenizers import Tokenizer, load_vocab\n",
        "from bert4keras.optimizers import Adam\n",
        "from bert4keras.snippets import sequence_padding, open\n",
        "from bert4keras.snippets import DataGenerator, AutoRegressiveDecoder\n",
        "#from tensorflow.keras.models import Model\n",
        "from keras.models import Model\n",
        "from rouge import Rouge  # pip install rouge\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "import model as md\n",
        "import utils\n",
        "\n",
        "# Load train data\n",
        "path_train = path_wd + path_datasets + '/CSL/train.json'\n",
        "path_dev = path_wd + path_datasets + '/CSL/dev.json'\n",
        "train_data = md.load_data(path_train)\n",
        "valid_data = md.load_data(path_dev)\n",
        "# Could use pandas, but pandas has limitation on cell string length\n",
        "\n",
        "# Initialize Model with Pretrained BERT\n",
        "token_dict, keep_tokens = load_vocab(\n",
        "    dict_path=dict_path,\n",
        "    simplified=True,\n",
        "    startswith=['[PAD]', '[UNK]', '[CLS]', '[SEP]'],\n",
        ")\n",
        "tokenizer = Tokenizer(token_dict, do_lower_case=True)\n",
        "model = md.build_transformer_model(\n",
        "    config_path,\n",
        "    checkpoint_path,\n",
        "    application='unilm',\n",
        "    keep_tokens=keep_tokens,  # 只保留keep_tokens中的字，精简原字\n",
        ")\n",
        "\n",
        "autotitle = AutoTitle(start_id=None, end_id=tokenizer._token_end_id, maxlen=32)\n",
        "evaluator = Evaluator()\n",
        "output = md.CrossEntropy(2)(model.inputs + model.outputs)\n",
        "model = md.Model(model.inputs, output)\n",
        "train_generator = md.data_generator(train_data, batch_size)\n",
        "model.compile(optimizer=md.Adam(1e-5))\n",
        "#model.summary()\n",
        "\n",
        "model.fit(\n",
        "    train_generator.forfit(),\n",
        "    steps_per_epoch=len(train_generator),\n",
        "    epochs=epochs,\n",
        "    callbacks=[evaluator]\n",
        ")\n",
        "\n",
        "# Save Trained Model\n",
        "path_models = '/models'\n",
        "utils.save_model_with_dt(path_wd + path_models, model, 'new_train_test')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-56-5498a52e9d5f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0mautotitle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_token_end_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0mevaluator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEvaluator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Evaluator' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHCi6ImzT9Zn"
      },
      "source": [
        "# LOAD TRAINED MODELS\n",
        "from tensorflow import keras\n",
        "from bert4keras.models import build_transformer_model\n",
        "\n",
        "path_models = '/models'\n",
        "\n",
        "# the Star Chaser Model trained with movie dataset only\n",
        "model_StarChaser = build_transformer_model(\n",
        "    config_path,\n",
        "    checkpoint_path,\n",
        "    application='unilm',\n",
        "    keep_tokens=keep_tokens,  # include only tokens in keep tokens\n",
        ")\n",
        "model_StarChaser.load_weights(path_wd + path_models + '/best_model_StarChaser.weights')\n",
        "\n",
        "# the Scholar Model trained with Computer Science Literature only\n",
        "model_Scholar = build_transformer_model(\n",
        "    config_path,\n",
        "    checkpoint_path,\n",
        "    application='unilm',\n",
        "    keep_tokens=keep_tokens,   # include only tokens in keep tokens\n",
        ")\n",
        "model_Scholar.load_weights(path_wd + path_models + '/best_model_Scholar.weights')\n",
        "\n",
        "# the Know-it-all Model Trained with both movie and cscl dataset\n",
        "model_Knowitall = build_transformer_model(\n",
        "    config_path,\n",
        "    checkpoint_path,\n",
        "    application='unilm',\n",
        "    keep_tokens=keep_tokens,   # include only tokens in keep tokens\n",
        ") \n",
        "model_Knowitall.load_weights(path_wd + path_models + '/best_model_Knowitall.weights')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zf8FM0X_UqAW"
      },
      "source": [
        "# PREDICTION\n",
        "\n",
        "# Load Library\n",
        "import json\n",
        "from model import AutoTitle\n",
        "\n",
        "path_predictions = '/predictions'\n",
        "path_test = path_wd + path_datasets + path_dataset_csl + '/test.json'\n",
        "path_prediction = path_wd + path_predictions + 'Model_StarChaser_predict_CSLTitle.json'\n",
        "test_data = []\n",
        "\n",
        "autotitle = AutoTitle(start_id=None, end_id=tokenizer._token_end_id, maxlen=32)\n",
        "\n",
        "with open(path_test, encoding='utf-8') as f:\n",
        "    for l in f:\n",
        "        test_data.append(json.loads(l.strip()))\n",
        "\n",
        "with open(path_prediction ,'w') as f:\n",
        "    for data in test_data:\n",
        "        pred_title = ''.join(autotitle.generate(data[\"abst\"], topk)).lower()\n",
        "        output = {}\n",
        "        output[\"id\"] = data[\"id\"]\n",
        "        output[\"title\"] = pred_title\n",
        "        f.write(json.dumps(output, ensure_ascii = False))\n",
        "        f.write('\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTk-bQxYUxn5"
      },
      "source": [
        "# RESULT ANALYSIS"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}